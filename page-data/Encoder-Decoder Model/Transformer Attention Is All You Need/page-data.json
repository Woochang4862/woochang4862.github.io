{"componentChunkName":"component---src-templates-post-jsx","path":"/Encoder-Decoder Model/Transformer Attention Is All You Need/","result":{"data":{"site":{"siteMetadata":{"title":"Woochang"}},"markdownRemark":{"id":"1826848d-1d60-5036-bd93-a8f5d05a1aff","excerpt":"Transformer : Attention Is All You Need Decoder ⇒ GPT-1 (2018), GPT-3 (2020) Encoder ⇒ BERT (NAACL 2019)","html":"<h2>Transformer : Attention Is All You Need</h2>\n<ul>\n<li>Decoder ⇒ <strong>GPT-1 (2018), GPT-3 (2020)</strong></li>\n<li>Encoder ⇒ <strong>BERT (NAACL 2019)</strong></li>\n</ul>","frontmatter":{"title":"Transformer : Attention Is All You Need","date":"June 27, 2022","update":"August 15, 2022","tags":["Encoder-Decoder Model"],"series":"Encoder-Decoder Model"},"fields":{"slug":"/Encoder-Decoder Model/Transformer Attention Is All You Need/","readingTime":{"minutes":0.105}}},"seriesList":{"edges":[{"node":{"id":"1826848d-1d60-5036-bd93-a8f5d05a1aff","fields":{"slug":"/Encoder-Decoder Model/Transformer Attention Is All You Need/"},"frontmatter":{"title":"Transformer : Attention Is All You Need"}}},{"node":{"id":"fe914b81-9363-54de-8e95-a19fd1a06e30","fields":{"slug":"/Encoder-Decoder Model/Attention/"},"frontmatter":{"title":"Attention"}}},{"node":{"id":"aa55587d-9d42-59a1-abec-729eb8b9b4fd","fields":{"slug":"/Encoder-Decoder Model/Seq2Seq/"},"frontmatter":{"title":"Seq2Seq"}}}]},"previous":{"fields":{"slug":"/Encoder-Decoder Model/"},"frontmatter":{"title":"Encoder-Decoder Model"}},"next":{"fields":{"slug":"/Encoder-Decoder Model/Attention/"},"frontmatter":{"title":"Attention"}}},"pageContext":{"id":"1826848d-1d60-5036-bd93-a8f5d05a1aff","series":"Encoder-Decoder Model","previousPostId":"e39b99ae-1931-5474-b8ac-ef48fddbe6c8","nextPostId":"fe914b81-9363-54de-8e95-a19fd1a06e30"}},"staticQueryHashes":[]}