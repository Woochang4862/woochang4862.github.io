{"componentChunkName":"component---src-pages-index-jsx","path":"/","result":{"data":{"site":{"siteMetadata":{"title":"Woochang"}},"allMarkdownRemark":{"group":[{"fieldValue":"AI","totalCount":1},{"fieldValue":"Data Science","totalCount":1},{"fieldValue":"Encoder-Decoder Model","totalCount":3},{"fieldValue":"Machine Learning","totalCount":1},{"fieldValue":"Sequencial Data","totalCount":1}],"nodes":[{"excerpt":"Attention","fields":{"slug":"/Encoder-Decoder Model/Attention/"},"frontmatter":{"date":"July 10, 2022","update":"Jul 10, 2022","title":"Attention","tags":["Encoder-Decoder Model"]}},{"excerpt":"Seq2Seq Sequence to Sequence Learning with Neural Networks (NIPS 2014) https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf RNN(1986)에서 파생된 LSTM(1997)을 활용한 Seq2Se…","fields":{"slug":"/Encoder-Decoder Model/Seq2Seq/"},"frontmatter":{"date":"July 10, 2022","update":"Jul 10, 2022","title":"Seq2Seq","tags":["Encoder-Decoder Model","Sequencial Data"]}},{"excerpt":"Encoder-Decoder Model Seq2Seq Attention Transformer : Attention Is All You Need","fields":{"slug":"/Encoder-Decoder Model/"},"frontmatter":{"date":"June 27, 2022","update":"Aug 15, 2022","title":"Encoder-Decoder Model","tags":["AI","Machine Learning","Data Science"]}},{"excerpt":"Transformer : Attention Is All You Need Decoder ⇒ GPT-1 (2018), GPT-3 (2020) Encoder ⇒ BERT (NAACL 2019)","fields":{"slug":"/Encoder-Decoder Model/Transformer Attention Is All You Need/"},"frontmatter":{"date":"June 27, 2022","update":"Aug 15, 2022","title":"Transformer : Attention Is All You Need","tags":["Encoder-Decoder Model"]}}]}},"pageContext":{}},"staticQueryHashes":[]}